Mon May 17 13:56:58 BST 2021
cgpu04.maxwell.local

Currently Loaded Modules:
  1) slurm/current   2) cudatoolkit-10.1.168

 

/tmp/slurmd/job786603/slurm_script: line 20: activate: No such file or directory
Mon May 17 13:57:00 2021       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 450.51.05    Driver Version: 450.51.05    CUDA Version: 11.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  GeForce RTX 208...  Off  | 00000000:2F:00.0 Off |                  N/A |
| 33%   54C    P0    70W / 250W |      0MiB / 11019MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   1  GeForce RTX 208...  Off  | 00000000:86:00.0 Off |                  N/A |
| 37%   45C    P0    41W / 250W |      0MiB / 11019MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+
PatchWiseModel(
  (features): Sequential(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
    (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): ReLU(inplace=True)
    (6): Conv2d(16, 16, kernel_size=(2, 2), stride=(2, 2))
    (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): ReLU(inplace=True)
    (9): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (10): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (11): ReLU(inplace=True)
    (12): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (13): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (14): ReLU(inplace=True)
    (15): Conv2d(32, 32, kernel_size=(2, 2), stride=(2, 2))
    (16): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (17): ReLU(inplace=True)
    (18): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (19): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (20): ReLU(inplace=True)
    (21): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (22): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (23): ReLU(inplace=True)
    (24): Conv2d(64, 64, kernel_size=(2, 2), stride=(2, 2))
    (25): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (26): ReLU(inplace=True)
    (27): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (28): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (29): ReLU(inplace=True)
    (30): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (31): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (32): ReLU(inplace=True)
    (33): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (34): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (35): ReLU(inplace=True)
    (36): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (37): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (38): ReLU(inplace=True)
    (39): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (40): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (41): ReLU(inplace=True)
    (42): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (43): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (44): ReLU(inplace=True)
    (45): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))
  )
  (classifier): Sequential(
    (0): Linear(in_features=12288, out_features=3, bias=True)
  )
)
Parameters: 1978662
Trainable parameters: 1978662
Using: cuda
Start training network: 2021/05/17 13:57

Using  22575 training samples
Using  4830 validation samples
Epoch 1/60
----------
train Loss: 2.5809 Acc: 0.3677
val Loss: 1.1325 Acc: 0.3551
Epoch 2/60
----------
train Loss: 1.1667 Acc: 0.3828
val Loss: 1.1137 Acc: 0.3551
Epoch 3/60
----------
train Loss: 1.3302 Acc: 0.3762
val Loss: 1.1028 Acc: 0.3551
Epoch 4/60
----------
train Loss: 1.0900 Acc: 0.3900
val Loss: 1.1292 Acc: 0.3551
Epoch 5/60
----------
train Loss: 1.0908 Acc: 0.3844
val Loss: 1.1263 Acc: 0.3551
Epoch 6/60
----------
train Loss: 1.0821 Acc: 0.3989
val Loss: 1.1331 Acc: 0.3551
Epoch 7/60
----------
train Loss: 1.0781 Acc: 0.4144
val Loss: 1.2172 Acc: 0.3551
Epoch 8/60
----------
train Loss: 1.0605 Acc: 0.4334
val Loss: 1.1941 Acc: 0.3551
Epoch 9/60
----------
train Loss: 1.0543 Acc: 0.4381
val Loss: 1.0726 Acc: 0.4246
Epoch 10/60
----------
train Loss: 1.0488 Acc: 0.4462
val Loss: 1.1687 Acc: 0.4164
Epoch 11/60
----------
train Loss: 1.0310 Acc: 0.4598
val Loss: 1.1034 Acc: 0.4375
Epoch 12/60
----------
train Loss: 1.0431 Acc: 0.4493
val Loss: 1.0983 Acc: 0.4379
Epoch 13/60
----------
train Loss: 1.0390 Acc: 0.4567
val Loss: 1.2493 Acc: 0.3768
Epoch 14/60
----------
train Loss: 1.1309 Acc: 0.4348
val Loss: 1.1483 Acc: 0.2681
Epoch 15/60
----------
train Loss: 1.0966 Acc: 0.3802
val Loss: 1.1361 Acc: 0.3551
Epoch 16/60
----------
train Loss: 1.0958 Acc: 0.3849
val Loss: 1.1198 Acc: 0.3551
Epoch 17/60
----------
train Loss: 1.1038 Acc: 0.3755
val Loss: 1.1171 Acc: 0.3551
Epoch 18/60
----------
train Loss: 1.1258 Acc: 0.3705
val Loss: 1.1197 Acc: 0.2681
Epoch 19/60
----------
train Loss: 1.1053 Acc: 0.3779
val Loss: 1.1016 Acc: 0.3551
Epoch 20/60
----------
train Loss: 1.1246 Acc: 0.3715
val Loss: 1.0977 Acc: 0.3768
Epoch 21/60
----------
train Loss: 1.0876 Acc: 0.3936
val Loss: 1.1084 Acc: 0.3551
Epoch 22/60
----------
train Loss: 1.0873 Acc: 0.3953
val Loss: 1.1140 Acc: 0.3551
Epoch 23/60
----------
train Loss: 1.0871 Acc: 0.3953
val Loss: 1.1180 Acc: 0.3551
Epoch 24/60
----------
train Loss: 1.0871 Acc: 0.3953
val Loss: 1.1156 Acc: 0.3551
Epoch 25/60
----------
train Loss: 1.0877 Acc: 0.3949
val Loss: 1.1070 Acc: 0.3551
Epoch 26/60
----------
train Loss: 1.0881 Acc: 0.3935
val Loss: 1.1161 Acc: 0.3551
Epoch 27/60
----------
train Loss: 1.0882 Acc: 0.3932
val Loss: 1.1124 Acc: 0.3551
Epoch 28/60
----------
train Loss: 1.0878 Acc: 0.3936
val Loss: 1.1175 Acc: 0.3551
Epoch 29/60
----------
train Loss: 1.0888 Acc: 0.3926
val Loss: 1.1365 Acc: 0.3551
Epoch 30/60
----------
train Loss: 1.0890 Acc: 0.3911
val Loss: 1.1383 Acc: 0.3551
Epoch 31/60
----------
train Loss: 1.0882 Acc: 0.3951
val Loss: 1.1168 Acc: 0.3551
Epoch 32/60
----------
train Loss: 1.0888 Acc: 0.3910
val Loss: 1.1125 Acc: 0.3551
Epoch 33/60
----------
train Loss: 1.0884 Acc: 0.3911
val Loss: 1.1094 Acc: 0.3551
Epoch 34/60
----------
train Loss: 1.0879 Acc: 0.3953
val Loss: 1.1129 Acc: 0.3551
Epoch 35/60
----------
train Loss: 1.0885 Acc: 0.3922
val Loss: 1.1166 Acc: 0.3551
Epoch 36/60
----------
train Loss: 1.0884 Acc: 0.3930
val Loss: 1.1258 Acc: 0.3551
Epoch 37/60
----------
train Loss: 1.0879 Acc: 0.3949
val Loss: 1.1121 Acc: 0.3551
Epoch 38/60
----------
train Loss: 1.0886 Acc: 0.3932
val Loss: 1.1354 Acc: 0.3551
Epoch 39/60
----------
train Loss: 1.0881 Acc: 0.3921
val Loss: 1.1174 Acc: 0.3551
Epoch 40/60
----------
train Loss: 1.0881 Acc: 0.3940
val Loss: 1.1159 Acc: 0.3551
Epoch 41/60
----------
train Loss: 1.0870 Acc: 0.3953
val Loss: 1.1157 Acc: 0.3551
Epoch 42/60
----------
train Loss: 1.0870 Acc: 0.3953
val Loss: 1.1158 Acc: 0.3551
Epoch 43/60
----------
train Loss: 1.0870 Acc: 0.3953
val Loss: 1.1158 Acc: 0.3551
Epoch 44/60
----------
train Loss: 1.0870 Acc: 0.3953
val Loss: 1.1157 Acc: 0.3551
Epoch 45/60
----------
train Loss: 1.0870 Acc: 0.3953
val Loss: 1.1160 Acc: 0.3551
Epoch 46/60
----------
train Loss: 1.0871 Acc: 0.3953
val Loss: 1.1156 Acc: 0.3551
Epoch 47/60
----------
train Loss: 1.0870 Acc: 0.3953
val Loss: 1.1196 Acc: 0.3551
Epoch 48/60
----------
slurmstepd: error: *** JOB 786603 ON cgpu04 CANCELLED AT 2021-05-18T13:57:14 DUE TO TIME LIMIT ***
