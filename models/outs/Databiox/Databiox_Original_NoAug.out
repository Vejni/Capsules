Mon May 17 10:44:55 BST 2021
cgpu02.maxwell.local

Currently Loaded Modules:
  1) slurm/current   2) cudatoolkit-10.1.168

 

/tmp/slurmd/job786573/slurm_script: line 20: activate: No such file or directory
Mon May 17 10:44:58 2021       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 450.51.05    Driver Version: 450.51.05    CUDA Version: 11.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  GeForce RTX 208...  Off  | 00000000:2F:00.0 Off |                  N/A |
| 30%   49C    P0    64W / 250W |      0MiB / 11019MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   1  GeForce RTX 208...  Off  | 00000000:86:00.0 Off |                  N/A |
| 37%   46C    P0    46W / 250W |      0MiB / 11019MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+
PatchWiseModel(
  (features): Sequential(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
    (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): ReLU(inplace=True)
    (6): Conv2d(16, 16, kernel_size=(2, 2), stride=(2, 2))
    (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): ReLU(inplace=True)
    (9): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (10): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (11): ReLU(inplace=True)
    (12): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (13): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (14): ReLU(inplace=True)
    (15): Conv2d(32, 32, kernel_size=(2, 2), stride=(2, 2))
    (16): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (17): ReLU(inplace=True)
    (18): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (19): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (20): ReLU(inplace=True)
    (21): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (22): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (23): ReLU(inplace=True)
    (24): Conv2d(64, 64, kernel_size=(2, 2), stride=(2, 2))
    (25): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (26): ReLU(inplace=True)
    (27): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (28): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (29): ReLU(inplace=True)
    (30): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (31): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (32): ReLU(inplace=True)
    (33): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (34): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (35): ReLU(inplace=True)
    (36): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (37): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (38): ReLU(inplace=True)
    (39): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (40): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (41): ReLU(inplace=True)
    (42): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (43): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (44): ReLU(inplace=True)
    (45): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))
  )
  (classifier): Sequential(
    (0): Linear(in_features=12288, out_features=3, bias=True)
  )
)
Parameters: 1978662
Trainable parameters: 1978662
Using: cuda
Start training network: 2021/05/17 10:45

Using  22575 training samples
Using  4830 validation samples
Epoch 1/60
----------
train Loss: 1.0609 Acc: 0.4422
val Loss: 1.2668 Acc: 0.3582
Epoch 2/60
----------
train Loss: 1.0164 Acc: 0.4595
val Loss: 1.0769 Acc: 0.4439
Epoch 3/60
----------
train Loss: 0.9854 Acc: 0.4852
val Loss: 1.1049 Acc: 0.4219
Epoch 4/60
----------
train Loss: 0.9748 Acc: 0.4963
val Loss: 1.1144 Acc: 0.4462
Epoch 5/60
----------
train Loss: 0.9502 Acc: 0.5185
val Loss: 1.0548 Acc: 0.4379
Epoch 6/60
----------
train Loss: 0.9372 Acc: 0.5318
val Loss: 1.0588 Acc: 0.4569
Epoch 7/60
----------
train Loss: 0.9157 Acc: 0.5516
val Loss: 1.0424 Acc: 0.4648
Epoch 8/60
----------
train Loss: 0.8914 Acc: 0.5733
val Loss: 1.1132 Acc: 0.4418
Epoch 9/60
----------
train Loss: 0.8661 Acc: 0.5943
val Loss: 1.0936 Acc: 0.4814
Epoch 10/60
----------
train Loss: 0.8508 Acc: 0.6041
val Loss: 1.1162 Acc: 0.4737
Epoch 11/60
----------
train Loss: 0.8192 Acc: 0.6268
val Loss: 1.2297 Acc: 0.4578
Epoch 12/60
----------
train Loss: 0.8000 Acc: 0.6370
val Loss: 1.0867 Acc: 0.5182
Epoch 13/60
----------
train Loss: 0.7848 Acc: 0.6458
val Loss: 1.1765 Acc: 0.4797
Epoch 14/60
----------
train Loss: 0.7716 Acc: 0.6567
val Loss: 1.1212 Acc: 0.5118
Epoch 15/60
----------
train Loss: 0.7440 Acc: 0.6673
val Loss: 1.2069 Acc: 0.5166
Epoch 16/60
----------
train Loss: 0.7317 Acc: 0.6768
val Loss: 1.0495 Acc: 0.5240
Epoch 17/60
----------
train Loss: 0.7446 Acc: 0.6781
val Loss: 1.0984 Acc: 0.5014
Epoch 18/60
----------
train Loss: 0.7311 Acc: 0.6807
val Loss: 1.0763 Acc: 0.5460
Epoch 19/60
----------
train Loss: 0.6934 Acc: 0.6982
val Loss: 1.1834 Acc: 0.5186
Epoch 20/60
----------
train Loss: 0.6822 Acc: 0.7038
val Loss: 1.1767 Acc: 0.4992
Epoch 21/60
----------
train Loss: 0.6016 Acc: 0.7439
val Loss: 1.1415 Acc: 0.5482
Epoch 22/60
----------
train Loss: 0.5797 Acc: 0.7559
val Loss: 1.1308 Acc: 0.5460
Epoch 23/60
----------
train Loss: 0.5689 Acc: 0.7600
val Loss: 1.1627 Acc: 0.5422
Epoch 24/60
----------
train Loss: 0.5650 Acc: 0.7616
val Loss: 1.2014 Acc: 0.5495
Epoch 25/60
----------
train Loss: 0.5557 Acc: 0.7672
val Loss: 1.1258 Acc: 0.5536
Epoch 26/60
----------
train Loss: 0.5538 Acc: 0.7657
val Loss: 1.1472 Acc: 0.5431
Epoch 27/60
----------
train Loss: 0.5458 Acc: 0.7704
val Loss: 1.1517 Acc: 0.5470
Epoch 28/60
----------
train Loss: 0.5362 Acc: 0.7719
val Loss: 1.1506 Acc: 0.5389
Epoch 29/60
----------
train Loss: 0.5358 Acc: 0.7751
val Loss: 1.1578 Acc: 0.5455
Epoch 30/60
----------
train Loss: 0.5257 Acc: 0.7796
val Loss: 1.1964 Acc: 0.5379
Epoch 31/60
----------
train Loss: 0.5223 Acc: 0.7796
val Loss: 1.2246 Acc: 0.5366
Epoch 32/60
----------
train Loss: 0.5172 Acc: 0.7834
val Loss: 1.2113 Acc: 0.5333
Epoch 33/60
----------
train Loss: 0.5198 Acc: 0.7828
val Loss: 1.2296 Acc: 0.5280
Epoch 34/60
----------
train Loss: 0.5136 Acc: 0.7869
val Loss: 1.1919 Acc: 0.5505
Epoch 35/60
----------
train Loss: 0.5131 Acc: 0.7838
val Loss: 1.1665 Acc: 0.5545
Epoch 36/60
----------
train Loss: 0.5115 Acc: 0.7867
val Loss: 1.1851 Acc: 0.5449
Epoch 37/60
----------
train Loss: 0.5064 Acc: 0.7909
val Loss: 1.1493 Acc: 0.5522
Epoch 38/60
----------
train Loss: 0.4978 Acc: 0.7946
val Loss: 1.2401 Acc: 0.5402
Epoch 39/60
----------
train Loss: 0.4976 Acc: 0.7936
val Loss: 1.2590 Acc: 0.5381
Epoch 40/60
----------
train Loss: 0.4924 Acc: 0.7951
val Loss: 1.1649 Acc: 0.5468
Epoch 41/60
----------
train Loss: 0.4799 Acc: 0.8026
val Loss: 1.2111 Acc: 0.5497
Epoch 42/60
----------
train Loss: 0.4755 Acc: 0.8003
val Loss: 1.2060 Acc: 0.5553
Epoch 43/60
----------
train Loss: 0.4770 Acc: 0.8005
val Loss: 1.2226 Acc: 0.5489
Epoch 44/60
----------
train Loss: 0.4697 Acc: 0.8073
val Loss: 1.1919 Acc: 0.5540
Epoch 45/60
----------
train Loss: 0.4748 Acc: 0.8037
val Loss: 1.2251 Acc: 0.5484
Epoch 46/60
----------
train Loss: 0.4724 Acc: 0.8036
val Loss: 1.1930 Acc: 0.5623
Epoch 47/60
----------
train Loss: 0.4761 Acc: 0.8030
val Loss: 1.2223 Acc: 0.5528
Epoch 48/60
----------
slurmstepd: error: *** JOB 786573 ON cgpu02 CANCELLED AT 2021-05-18T10:45:14 DUE TO TIME LIMIT ***
