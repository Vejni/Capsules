Mon May 17 10:44:55 BST 2021
cgpu03.maxwell.local

Currently Loaded Modules:
  1) slurm/current   2) cudatoolkit-10.1.168

 

/tmp/slurmd/job786574/slurm_script: line 20: activate: No such file or directory
Mon May 17 10:44:58 2021       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 450.51.05    Driver Version: 450.51.05    CUDA Version: 11.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  GeForce RTX 208...  Off  | 00000000:2F:00.0 Off |                  N/A |
| 31%   47C    P0    60W / 250W |      0MiB / 11019MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   1  GeForce RTX 208...  Off  | 00000000:86:00.0 Off |                  N/A |
| 37%   46C    P0    38W / 250W |      0MiB / 11019MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+
PatchWiseModel(
  (features): Sequential(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
    (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): ReLU(inplace=True)
    (6): Conv2d(16, 16, kernel_size=(2, 2), stride=(2, 2))
    (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): ReLU(inplace=True)
    (9): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (10): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (11): ReLU(inplace=True)
    (12): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (13): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (14): ReLU(inplace=True)
    (15): Conv2d(32, 32, kernel_size=(2, 2), stride=(2, 2))
    (16): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (17): ReLU(inplace=True)
    (18): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (19): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (20): ReLU(inplace=True)
    (21): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (22): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (23): ReLU(inplace=True)
    (24): Conv2d(64, 64, kernel_size=(2, 2), stride=(2, 2))
    (25): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (26): ReLU(inplace=True)
    (27): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (28): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (29): ReLU(inplace=True)
    (30): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (31): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (32): ReLU(inplace=True)
    (33): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (34): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (35): ReLU(inplace=True)
    (36): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (37): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (38): ReLU(inplace=True)
    (39): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (40): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (41): ReLU(inplace=True)
    (42): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (43): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (44): ReLU(inplace=True)
    (45): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))
  )
  (classifier): Sequential(
    (0): Linear(in_features=12288, out_features=3, bias=True)
  )
)
Parameters: 1978662
Trainable parameters: 1978662
Using: cuda
Start training network: 2021/05/17 10:45

Using  22575 training samples
Using  4830 validation samples
Epoch 1/60
----------
train Loss: 1.0569 Acc: 0.4477
val Loss: 1.2207 Acc: 0.4104
Epoch 2/60
----------
train Loss: 1.0143 Acc: 0.4559
val Loss: 1.1676 Acc: 0.3805
Epoch 3/60
----------
train Loss: 0.9981 Acc: 0.4673
val Loss: 1.1686 Acc: 0.3872
Epoch 4/60
----------
train Loss: 1.0016 Acc: 0.4707
val Loss: 1.0871 Acc: 0.4420
Epoch 5/60
----------
train Loss: 0.9696 Acc: 0.4968
val Loss: 1.0974 Acc: 0.4501
Epoch 6/60
----------
train Loss: 0.9507 Acc: 0.5211
val Loss: 1.0959 Acc: 0.4549
Epoch 7/60
----------
train Loss: 0.9304 Acc: 0.5417
val Loss: 1.1247 Acc: 0.4306
Epoch 8/60
----------
train Loss: 0.9053 Acc: 0.5656
val Loss: 1.1178 Acc: 0.4623
Epoch 9/60
----------
train Loss: 0.8851 Acc: 0.5833
val Loss: 1.1822 Acc: 0.4743
Epoch 10/60
----------
train Loss: 0.8679 Acc: 0.5920
val Loss: 1.1211 Acc: 0.4542
Epoch 11/60
----------
train Loss: 0.8337 Acc: 0.6172
val Loss: 1.2120 Acc: 0.4580
Epoch 12/60
----------
train Loss: 0.8214 Acc: 0.6272
val Loss: 1.0111 Acc: 0.5271
Epoch 13/60
----------
train Loss: 0.7966 Acc: 0.6383
val Loss: 1.1294 Acc: 0.4901
Epoch 14/60
----------
train Loss: 0.7816 Acc: 0.6460
val Loss: 1.0326 Acc: 0.5072
Epoch 15/60
----------
train Loss: 0.7630 Acc: 0.6563
val Loss: 1.1538 Acc: 0.4952
Epoch 16/60
----------
train Loss: 0.7471 Acc: 0.6684
val Loss: 1.0765 Acc: 0.5151
Epoch 17/60
----------
train Loss: 0.7324 Acc: 0.6725
val Loss: 1.0660 Acc: 0.5101
Epoch 18/60
----------
train Loss: 0.7352 Acc: 0.6753
val Loss: 1.0335 Acc: 0.5366
Epoch 19/60
----------
train Loss: 0.7001 Acc: 0.6905
val Loss: 1.0403 Acc: 0.5176
Epoch 20/60
----------
train Loss: 0.6889 Acc: 0.6966
val Loss: 1.0956 Acc: 0.5137
Epoch 21/60
----------
train Loss: 0.6090 Acc: 0.7372
val Loss: 1.0789 Acc: 0.5420
Epoch 22/60
----------
train Loss: 0.5899 Acc: 0.7466
val Loss: 1.0841 Acc: 0.5433
Epoch 23/60
----------
train Loss: 0.5754 Acc: 0.7523
val Loss: 1.1052 Acc: 0.5369
Epoch 24/60
----------
train Loss: 0.5713 Acc: 0.7551
val Loss: 1.1143 Acc: 0.5501
Epoch 25/60
----------
train Loss: 0.5601 Acc: 0.7612
val Loss: 1.0962 Acc: 0.5501
Epoch 26/60
----------
train Loss: 0.5549 Acc: 0.7632
val Loss: 1.0973 Acc: 0.5540
Epoch 27/60
----------
train Loss: 0.5505 Acc: 0.7680
val Loss: 1.1137 Acc: 0.5431
Epoch 28/60
----------
train Loss: 0.5430 Acc: 0.7669
val Loss: 1.0739 Acc: 0.5596
Epoch 29/60
----------
train Loss: 0.5406 Acc: 0.7738
val Loss: 1.1478 Acc: 0.5445
Epoch 30/60
----------
train Loss: 0.5351 Acc: 0.7727
val Loss: 1.1252 Acc: 0.5474
Epoch 31/60
----------
train Loss: 0.5279 Acc: 0.7755
val Loss: 1.1492 Acc: 0.5451
Epoch 32/60
----------
train Loss: 0.5269 Acc: 0.7758
val Loss: 1.1155 Acc: 0.5507
Epoch 33/60
----------
train Loss: 0.5210 Acc: 0.7771
val Loss: 1.1823 Acc: 0.5480
Epoch 34/60
----------
train Loss: 0.5183 Acc: 0.7814
val Loss: 1.1207 Acc: 0.5549
Epoch 35/60
----------
train Loss: 0.5163 Acc: 0.7825
val Loss: 1.1050 Acc: 0.5623
Epoch 36/60
----------
train Loss: 0.5127 Acc: 0.7841
val Loss: 1.1308 Acc: 0.5561
Epoch 37/60
----------
train Loss: 0.5093 Acc: 0.7844
val Loss: 1.1692 Acc: 0.5489
Epoch 38/60
----------
train Loss: 0.5001 Acc: 0.7895
val Loss: 1.1858 Acc: 0.5501
Epoch 39/60
----------
train Loss: 0.4998 Acc: 0.7890
val Loss: 1.1837 Acc: 0.5441
Epoch 40/60
----------
train Loss: 0.4938 Acc: 0.7946
val Loss: 1.1784 Acc: 0.5414
Epoch 41/60
----------
train Loss: 0.4795 Acc: 0.7974
val Loss: 1.1768 Acc: 0.5530
Epoch 42/60
----------
train Loss: 0.4791 Acc: 0.8008
val Loss: 1.1666 Acc: 0.5609
Epoch 43/60
----------
train Loss: 0.4755 Acc: 0.8041
val Loss: 1.1983 Acc: 0.5528
Epoch 44/60
----------
train Loss: 0.4733 Acc: 0.8023
val Loss: 1.1594 Acc: 0.5536
Epoch 45/60
----------
train Loss: 0.4728 Acc: 0.8040
val Loss: 1.2030 Acc: 0.5592
Epoch 46/60
----------
train Loss: 0.4718 Acc: 0.8017
val Loss: 1.1783 Acc: 0.5619
Epoch 47/60
----------
train Loss: 0.4765 Acc: 0.8005
val Loss: 1.2069 Acc: 0.5530
Epoch 48/60
----------
slurmstepd: error: *** JOB 786574 ON cgpu03 CANCELLED AT 2021-05-18T10:45:14 DUE TO TIME LIMIT ***
