{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PyTorchResNet18.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNSdq+93h0r79PsuEF8XmEY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vejni/Capsules_Thesis/blob/main/PyTorchResNet18.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kDzENDb2A7h3",
        "outputId": "7921a0c3-346a-41d9-ecc1-807302c3efb0"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GohAuY1fA_x3"
      },
      "source": [
        "import torch\r\n",
        "import numpy as np\r\n",
        "import random\r\n",
        "import time"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s5hW6SNfBB8u"
      },
      "source": [
        "def set_seed(seed):\r\n",
        "    torch.backends.cudnn.deterministic = True\r\n",
        "    torch.backends.cudnn.benchmark = False\r\n",
        "    torch.manual_seed(seed)\r\n",
        "    torch.cuda.manual_seed_all(seed)\r\n",
        "    np.random.seed(seed)\r\n",
        "    random.seed(seed)\r\n",
        "\r\n",
        "set_seed(123)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WmeXbckkBC9m",
        "outputId": "af443b02-3332-4bc3-d547-6bb2b0051906"
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\r\n",
        "\r\n",
        "# Assuming that we are on a CUDA machine, this should print a CUDA device:\r\n",
        "print(device)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NXgthh0aBD62"
      },
      "source": [
        "BATCH_SIZE = 32\r\n",
        "IMG_WIDTH = 228\r\n",
        "IMG_HEIGHT = 228\r\n",
        "CROPPED_SIZE = 128\r\n",
        "TRAINING_SET = 0.8\r\n",
        "TRAIN_DATA_PATH = \"./gdrive/MyDrive/Histopathological_Graded/\"\r\n",
        "EPOCHS = 100\r\n",
        "MODEL_NAME = \"PyTorch_ResNet18_Epochs100\" + str(int(time.time()))"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dsbrx6CVBFIO"
      },
      "source": [
        "class MapDataset(torch.utils.data.Dataset):\r\n",
        "    \"\"\"\r\n",
        "    Given a dataset, creates a dataset which applies a mapping function\r\n",
        "    to its items (lazily, only when an item is called).\r\n",
        "\r\n",
        "    Note that data is not cloned/copied from the initial dataset.\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    def __init__(self, dataset, map_fn):\r\n",
        "        self.dataset = dataset\r\n",
        "        self.map = map_fn\r\n",
        "\r\n",
        "    def __getitem__(self, index):\r\n",
        "        if self.map:     \r\n",
        "            x = self.map(self.dataset[index][0]) \r\n",
        "        else:     \r\n",
        "            x = self.dataset[index][0]  # image\r\n",
        "        y = self.dataset[index][1]   # label      \r\n",
        "        return x, y\r\n",
        "\r\n",
        "    def __len__(self):\r\n",
        "        return len(self.dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VqM99nOpBGgW"
      },
      "source": [
        "import os\r\n",
        "import torchvision\r\n",
        "import torchvision.transforms as transforms\r\n",
        "from torch.utils.data import Subset, DataLoader\r\n",
        "import PIL\r\n",
        "\r\n",
        "training_transforms = transforms.Compose([\r\n",
        "    transforms.Resize((IMG_WIDTH,IMG_HEIGHT)),\r\n",
        "    transforms.CenterCrop(CROPPED_SIZE),\r\n",
        "    transforms.RandomHorizontalFlip(),\r\n",
        "    transforms.ColorJitter(hue=.05, saturation=.05),\r\n",
        "    transforms.RandomVerticalFlip(),\r\n",
        "    transforms.RandomRotation(10, resample=PIL.Image.BILINEAR),\r\n",
        "    transforms.ToTensor(),\r\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5],\r\n",
        "                         std=[0.5, 0.5, 0.5] )\r\n",
        "    ])\r\n",
        "\r\n",
        "validation_transforms = transforms.Compose([\r\n",
        "    transforms.Resize((IMG_WIDTH, IMG_HEIGHT)),\r\n",
        "    transforms.CenterCrop(CROPPED_SIZE),\r\n",
        "    transforms.ToTensor(),\r\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5],\r\n",
        "                         std=[0.5, 0.5, 0.5] )\r\n",
        "    ])\r\n",
        "\r\n",
        "data = torchvision.datasets.ImageFolder(root=TRAIN_DATA_PATH, transform=None)\r\n",
        "\r\n",
        "train_size = int(TRAINING_SET*len(data))\r\n",
        "val_size = len(data) - train_size\r\n",
        "train_data, validation_data = torch.utils.data.random_split(data, [train_size, val_size])\r\n",
        "\r\n",
        "train_data = MapDataset(train_data, training_transforms)\r\n",
        "validation_data = MapDataset(validation_data, training_transforms)\r\n",
        "\r\n",
        "train_data_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True,  num_workers=0)\r\n",
        "val_data_loader = DataLoader(validation_data, batch_size=BATCH_SIZE, shuffle=True,  num_workers=0)\r\n",
        "\r\n",
        "print(len(train_data), len(validation_data))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HgkeWpTGBHz2"
      },
      "source": [
        "import matplotlib.pyplot as plt\r\n",
        "%matplotlib inline\r\n",
        "plt.rcParams['figure.figsize'] = [12, 8]\r\n",
        "import random\r\n",
        "\r\n",
        "index = random.randint(0, 31)\r\n",
        "temp = next(iter(train_data_loader))\r\n",
        "img = temp[0][index]\r\n",
        "label = temp[1][index]\r\n",
        "\r\n",
        "npimg = img.numpy()\r\n",
        "plt.imshow(np.transpose(npimg, (1, 2, 0)))\r\n",
        "plt.show()\r\n",
        "\r\n",
        "print(label)\r\n",
        "print(img.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2CG7iHfkBJEG"
      },
      "source": [
        "def imshow(img):\r\n",
        "    npimg = img.numpy()\r\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\r\n",
        "    plt.show()\r\n",
        "\r\n",
        "dataiter = iter(train_data_loader)\r\n",
        "images, labels = dataiter.next()\r\n",
        "imshow(torchvision.utils.make_grid(images))\r\n",
        "labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mKLImBgVBLhO"
      },
      "source": [
        "######################################################################################################### ResNet 18 ###############################################################################################################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MXIoykVaBMZm"
      },
      "source": [
        "from torchvision import models\r\n",
        "from torch.optim import lr_scheduler\r\n",
        "import torch.optim as optim\r\n",
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F\r\n",
        "import time\r\n",
        "import os\r\n",
        "import copy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6emyugGBNee"
      },
      "source": [
        "def train_model(model, criterion, optimizer, scheduler, num_epochs=EPOCHS):\r\n",
        "    since = time.time()\r\n",
        "\r\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\r\n",
        "    best_acc = 0.0\r\n",
        "    train_losses = []\r\n",
        "    valid_losses = []\r\n",
        "    train_acc = []\r\n",
        "    val_acc = []\r\n",
        "\r\n",
        "    for epoch in range(num_epochs):\r\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\r\n",
        "        print('-' * 10)\r\n",
        "\r\n",
        "        # Each epoch has a training and validation phase\r\n",
        "        for phase in ['train', 'val']:\r\n",
        "            if phase == 'train':\r\n",
        "                model.train()  # Set model to training mode\r\n",
        "                dataloader = train_data_loader\r\n",
        "            else:\r\n",
        "                model.eval()   # Set model to evaluate mode\r\n",
        "                dataloader = val_data_loader\r\n",
        "\r\n",
        "            running_loss = 0.0\r\n",
        "            running_corrects = 0\r\n",
        "\r\n",
        "            # Iterate over data.\r\n",
        "            for inputs, labels in dataloader:\r\n",
        "                inputs = inputs.to(device)\r\n",
        "                labels = labels.to(device)\r\n",
        "\r\n",
        "                # zero the parameter gradients\r\n",
        "                optimizer.zero_grad()\r\n",
        "\r\n",
        "                # forward\r\n",
        "                # track history if only in train\r\n",
        "                with torch.set_grad_enabled(phase == 'train'):\r\n",
        "                    outputs = model(inputs)\r\n",
        "                    _, preds = torch.max(outputs, 1)\r\n",
        "                    loss = criterion(outputs, labels)\r\n",
        "\r\n",
        "                    # backward + optimize only if in training phase\r\n",
        "                    if phase == 'train':\r\n",
        "                        loss.backward()\r\n",
        "                        optimizer.step()\r\n",
        "\r\n",
        "                # statistics\r\n",
        "                running_loss += loss.item() * inputs.size(0)\r\n",
        "                running_corrects += torch.sum(preds == labels.data)\r\n",
        "            if phase == 'train':\r\n",
        "                scheduler.step()\r\n",
        "\r\n",
        "            epoch_loss = running_loss /len(dataloader)\r\n",
        "            epoch_acc = running_corrects.double() / len(dataloader)\r\n",
        "\r\n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\r\n",
        "                phase, epoch_loss, epoch_acc))\r\n",
        "            \r\n",
        "            if phase == 'train':\r\n",
        "                train_losses.append(epoch_loss)\r\n",
        "                train_acc.append(epoch_acc)\r\n",
        "            else:\r\n",
        "                valid_losses.append(epoch_loss)\r\n",
        "                val_acc.append(epoch_acc)\r\n",
        "\r\n",
        "            # deep copy the model\r\n",
        "            if phase == 'val' and epoch_acc > best_acc:\r\n",
        "                best_acc = epoch_acc\r\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\r\n",
        "\r\n",
        "        print()\r\n",
        "\r\n",
        "    time_elapsed = time.time() - since\r\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(\r\n",
        "        time_elapsed // 60, time_elapsed % 60))\r\n",
        "    print('Best val Acc: {:4f}'.format(best_acc))\r\n",
        "\r\n",
        "    # load best model weights\r\n",
        "    model.load_state_dict(best_model_wts)\r\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "drVW5UYzBO5u"
      },
      "source": [
        "model_ft = models.resnet18(pretrained=True)\r\n",
        "\r\n",
        "for param in model_ft.parameters():\r\n",
        "    param.requires_grad = False\r\n",
        "\r\n",
        "num_ftrs = model_ft.fc.in_features\r\n",
        "# Here the size of each output sample is set to 2.\r\n",
        "# Alternatively, it can be generalized to nn.Linear(num_ftrs, len(class_names)).\r\n",
        "model_ft.fc = nn.Linear(num_ftrs, 3)\r\n",
        "\r\n",
        "\r\n",
        "for param in model_ft.fc.parameters():\r\n",
        "    param.requires_grad = True\r\n",
        "\r\n",
        "\r\n",
        "model_ft = model_ft.to(device)\r\n",
        "\r\n",
        "criterion = nn.CrossEntropyLoss()\r\n",
        "\r\n",
        "# Observe that all parameters are being optimized\r\n",
        "optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\r\n",
        "\r\n",
        "# Decay LR by a factor of 0.1 every 7 epochs\r\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x_fNeriZBQm2"
      },
      "source": [
        "model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler, num_epochs=EPOCHS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XnSEckMyBwsW"
      },
      "source": [
        "%config InlineBackend.figure_format = 'retina'\r\n",
        "\r\n",
        "plt.plot(train_losses, label='Training loss')\r\n",
        "plt.plot(valid_losses, label='Validation loss')\r\n",
        "plt.xlabel(\"Epochs\")\r\n",
        "plt.ylabel(\"Loss\")\r\n",
        "plt.legend(frameon=False)\r\n",
        "\r\n",
        "%config InlineBackend.figure_format = 'retina'\r\n",
        "\r\n",
        "plt.plot(train_acc, label='Training Accuracy')\r\n",
        "plt.plot(val_acc, label='Validation Accuracy')\r\n",
        "plt.xlabel(\"Epochs\")\r\n",
        "plt.ylabel(\"Acc\")\r\n",
        "plt.legend(frameon=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WcLrRWGuC5nv"
      },
      "source": [
        "import pickle\r\n",
        "\r\n",
        "\r\n",
        "with open(\"./gdrive/MyDrive/Models/Stats/\" + MODEL_NAME + \".txt\", \"wb\") as fp:\r\n",
        "    pickle.dump(train_losses, fp)\r\n",
        "    pickle.dump(train_acc, fp)\r\n",
        "    pickle.dump(valid_losses, fp)\r\n",
        "    pickle.dump(val_acc, fp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_tchJy4E0fn"
      },
      "source": [
        "class_correct = list(0. for i in range(3))\r\n",
        "class_total = list(0. for i in range(3))\r\n",
        "with torch.no_grad():\r\n",
        "    for images, labels in val_data_loader:\r\n",
        "        images = images.to(device)\r\n",
        "        labels = labels.to(device)\r\n",
        "        outputs = model(images)\r\n",
        "        _, predicted = torch.max(outputs, 1)\r\n",
        "        c = (predicted == labels).squeeze()\r\n",
        "        for i in range(4):\r\n",
        "            label = labels[i]\r\n",
        "            class_correct[label] += c[i].item()\r\n",
        "            class_total[label] += 1\r\n",
        "\r\n",
        "\r\n",
        "for i in range(3):\r\n",
        "    print('Accuracy of %5s : %2d %%' % (\r\n",
        "        i, 100 * class_correct[i] / class_total[i]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Awt-A4LXBbXG"
      },
      "source": [
        "# Save \r\n",
        "torch.save(model_ft.state_dict(), './gdrive/MyDrive/Models/' + MODEL_NAME + '.ckpt')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}